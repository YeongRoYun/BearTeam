{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b535b3",
   "metadata": {},
   "source": [
    "## 학습한 모델 저장하기\n",
    "> 일반적인 torch.save x -> yolov5 내부 클래스를 이용   \n",
    "> 최종 모델 yolov5 mAP 약 0.748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a47269c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "custom_YOLOv5n summary: 213 layers, 1763224 parameters, 0 gradients\n"
     ]
    }
   ],
   "source": [
    "ROOT = 'C:/Users/5788j/Desktop/yolov5_bear-team/'\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "import torch\n",
    "import time\n",
    "\n",
    "device = torch.device('cpu')\n",
    "weight_path = ROOT + 'last_best_weight.pt'\n",
    "data = ROOT +'/data/custom_data.yaml'\n",
    "\n",
    "dnn=False\n",
    "model = DetectMultiBackend(weight_path, device=device, dnn=dnn, data=data)\n",
    "torch.save(model, '../yolov5_bear-team/our_last_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f28dd",
   "metadata": {},
   "source": [
    "## detect 함수 커스텀\n",
    "> 기존 detect 함수는 쓸모없는 동작이 포함되어 추론시간이 길어져 fps down -> bear team 서비스에 맞게 커스텀 필요   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8d0bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import argparse\n",
    "# import platform\n",
    "# import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "# import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "# from models.common import DetectMultiBackend\n",
    "# from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
    "# from utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
    "#                            increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
    "# from utils.plots import Annotator, colors, save_one_box\n",
    "# from utils.torch_utils import select_device, smart_inference_mode\n",
    "# from utils.augmentations import letterbox\n",
    "from utils.general import non_max_suppression, scale_coords, xyxy2xywh\n",
    "# import torch\n",
    "# import time\n",
    "# import json\n",
    "# from collections import OrderedDict # 딕셔너리 형태인데 (key가 아닌 순서로 고려)\n",
    "import numpy as np\n",
    "\n",
    "weight_path = '../yolov5_bear-team/our_last_model.pt' # mAP 0.748\n",
    "ROOT = 'C:/Users/5788j/Desktop/yolov5_bear-team/'\n",
    "model = None\n",
    "\n",
    "# @torch.no_grad()\n",
    "def modified_detect(weight_path, input_img, start_token):\n",
    "    '''source는 추론할 이미지 경로, save_dir은 추론 이미지 저장 경로'''\n",
    "    \n",
    "    global model\n",
    "    weight_path = weight_path # 가중치 경로\n",
    "    input_img = input_img # 이미지 경로\n",
    "    device = torch.device('cpu')\n",
    "    conf_thres=0.25 \n",
    "    iou_thres=0.45 \n",
    "    classes=None \n",
    "    max_det = 500\n",
    "    hide_labels=False  # hide labels\n",
    "    hide_conf=False\n",
    "\n",
    "    # 이미지 사이즈 주기\n",
    "    imgsz=(640,640)\n",
    "\n",
    "    # load model ( modified )\n",
    "    if start_token == True:\n",
    "        model = torch.load(weight_path, map_location=device)\n",
    "        \n",
    "        \n",
    "    stride, names, pt = model.stride, model.names, model.pt\n",
    "    iou_thres = 0.5\n",
    "    agnostic_nms = False\n",
    "    max_det = 1000\n",
    "    save_crop = False\n",
    "    save_conf = True\n",
    "    line_thickness=3\n",
    "\n",
    "    # Run inference\n",
    "    # im0는 원본 이미지(BGR), im은 전처리하여 모델에 들어갈 이미지\n",
    "#     im0 = cv2.cvtColor(input_img, cv2.COLOR_RGB2BGR)\n",
    "    im0 = input_img\n",
    "#     im = letterbox(im0, 640, stride=32, auto=True)[0]  # padded resize\n",
    "    im = im0\n",
    "    im = im.transpose((2, 0, 1)) # [::-1]  # HWC to CHW, BGR to RGB\n",
    "    im = np.ascontiguousarray(im)  # contiguous\n",
    "#     seen, windows, dt = 0, [], (Profile(), Profile(), Profile()) # dt(delta time) -> [dt, dt, dt]\n",
    "    seen = 0\n",
    "# im은 전처리하여 모델에 넣을 이미지\n",
    "# im0는 raw이미지 (원래 해상도의 BGR)\n",
    "#     with dt[0]:\n",
    "    im = torch.from_numpy(im).to(torch.float32).to(device)\n",
    "#     im = im.float()#im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
    "    im /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]  # expand for batch dim\n",
    "    # Inference\n",
    "#     with dt[1]:\n",
    "    model.eval() # 이거는 init에 들어간다.\n",
    "    with torch.no_grad(): #이거는 데코레이터로\n",
    "        pred = model(im)\n",
    "\n",
    "    # NMS\n",
    "#     with dt[2]:\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)[0]\n",
    "#     det = pred[0]\n",
    "        # Process predictions\n",
    "#     for i, det in enumerate(pred):  # per image -> i는 이미지 개수, det는 pred값\n",
    "    seen += 1\n",
    "#         im0_copy =  im0.copy()\n",
    "# bbox unnorm을 위한 사이즈 gain            \n",
    "#         gn = torch.tensor(im0_copy.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "    gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "# annotation 준비\n",
    "#         annotator = Annotator(im0, line_width=line_thickness, example=str(names))        \n",
    "\n",
    "        # bbox, confidence, cls값 저장\n",
    "    inference_inform = []\n",
    "    if len(pred): # 디텍션한 객체 개수\n",
    "# 예측한 객체 개수 마다 bbox(det 1~4열)를 unnormalize 진행\n",
    "        pred[:, :4] = scale_coords(im.shape[2:], pred[:, :4], im0.shape).round()\n",
    "\n",
    "            # Print results\n",
    "#             for c in det[:, -1].unique():\n",
    "#                 n = (det[:, -1] == c).sum()  # detections per class\n",
    "\n",
    "        for *xyxy, conf, cls in reversed(pred):\n",
    "            xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1)  # normalized xywh\n",
    "#                 obj = Object('category': )\n",
    "            inference_inform.append({\n",
    "                                         'category' : int(cls),\n",
    "                                         'probability' : float(conf), \n",
    "                                         'center' : xywh[:2], \n",
    "                                         'width':xywh[-2],\n",
    "                                         'height':xywh[-1]\n",
    "                                                            })\n",
    "#                     c = int(cls)  # integer class\n",
    "#                     label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
    "#                     annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "\n",
    "#             # Stream results\n",
    "#             im0 = annotator.result() # 예측 정보를 전부 기재한 이미지        \n",
    "\n",
    "    return inference_inform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80124a0c",
   "metadata": {},
   "source": [
    "## Modified_Detect 함수 테스트  \n",
    "> 서비스에 맞게 빠른 서비스를 위해 커스텀한 detect함수 동작, fps를 테스트한다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23bc8f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 10.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "5it [00:00,  9.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:00,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:01,  9.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "9\n",
      "이미지 9장 추론 경과시간 :  1.0777127742767334\n",
      "최종 모델 fps : 9.279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "sources = list(glob.glob('../yolov5_bear-team/validation_img' + '/*'))\n",
    "\n",
    "result=[]\n",
    "start = time.time()\n",
    "for i, img in tqdm(enumerate(sources)): \n",
    "    \n",
    "    if i == 0:\n",
    "        start_token = True\n",
    "    else:\n",
    "        start_token = False\n",
    "    input_img = cv2.imread(img, cv2.COLOR_BGR2RGB)\n",
    "    result.append(modified_detect(weight_path, input_img, start_token))\n",
    "    print(i)\n",
    "    \n",
    "end = time.time()\n",
    "print(f'이미지 {i}장 추론 경과시간 : ', end-start)\n",
    "print(f'최종 모델 fps : {round( (i+1)/(end-start), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001401a",
   "metadata": {},
   "source": [
    "## testset으로 input / output 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "527eeff8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# category == class name\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mannotation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobj_key\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     18\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmotor cycle\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m annotation[obj_key][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not dict"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "# 랜덤 검수\n",
    "i = random.randint(0, len(glob.glob('../yolov5_bear-team/validation_img' + '/*'))-1)\n",
    "img_path = glob.glob('../yolov5_bear-team/validation_img' + '/*')[i]\n",
    "\n",
    "annotation = result[i] # img_path에 있는 모든 이미지에 대해 추론한 annotation이 담긴 리스트\n",
    "\n",
    "# 여러개여도1개의 객체만 bbox확인(test)\n",
    "for ii, obj_key in enumerate(annotation):\n",
    "    if ii != 0: # 객체 하나만 검수\n",
    "        break\n",
    "\n",
    "    # category == class name\n",
    "    if annotation[obj_key]['category'] == 0:\n",
    "        c = 'motor cycle' \n",
    "    elif annotation[obj_key]['category'] == 1:\n",
    "        c = 'bicycle' \n",
    "    elif annotation[obj_key]['category'] == 2:\n",
    "        c = 'kick board'\n",
    "        \n",
    "    raw_w, raw_h = cv2.imread(img_path).shape[:2]\n",
    "\n",
    "    x = annotation[obj_key]['center'][0] * raw_w\n",
    "    y = annotation[obj_key]['center'][1] * raw_h\n",
    "    w = annotation['obj_0']['width'] * raw_w\n",
    "    h = annotation['obj_0']['height'] * raw_h\n",
    "\n",
    "# arguments\n",
    "    color = (0,255,0)\n",
    "    text_pos = (int(x + w/2), int(y - h/2))\n",
    "    font_size = 25\n",
    "#     font = ImageFont.truetype(\"arial.ttf\", font_size) # arial.ttf 글씨체, font_size=15\n",
    "\n",
    "    # image\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.rectangle( (int(x - w/2), int(y+h/2), int(x + w/2), int(y - h/2)), outline=color, width = 3) # 좌상단, 우하단 좌표\n",
    "    draw.text(text_pos, c, color) \n",
    "img\n",
    "\n",
    "# testing_inference(img_path, annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9476edd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('Data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "89fbfe5c6536e7a994d5354a6654c5e12151a0c0d5383310530fa5743fb366f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
